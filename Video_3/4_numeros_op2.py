# -*- coding: utf-8 -*-
"""4_numeros_op2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BBBfKPZ81AW9pX0wShIK5KN93FsIni69
"""

import tensorflow as tf
import tensorflow_datasets as tfds


#Descargar set de datos de MINIST (Numeros escritos a mano, etiquetados)
datos, metadatos = tfds.load('mnist', as_supervised=True, with_info=True)

#Obtenemos en variables separados los datos de entrenamiento (60k) y pruebas (10k)
datos_entrenamiento, datos_pruebas=datos['train'], datos['test']

#Etiquetas de las 10 categorias posibles (simplemente son los números del 0 al 9)
nombres_clases=metadatos.features['label'].names

#Funcion de normalizacion para los datos (Pasar valor de los pixeles de 0-255 a 0-1)
#Hace que la red aprenda mejor y mas rapido
def normalizar(imagenes, etiquetas):
   imagenes=tf.cast(imagenes, tf.float32)
   imagenes /=255 #Aqui lo pasa de 0-255 a 0-1
   return imagenes, etiquetas

#Normalizar los datos de entrenamiento y pruenas con la funcion que hicimos
datos_entrenamiento=datos_entrenamiento.map(normalizar)
datos_pruebas=datos_pruebas.map(normalizar)

#Agregar a cache (usar memoria en lugar de disco, entrenamiento mas rapido)
datos_entrenamiento=datos_entrenamiento.cache()
datos_pruebas=datos_pruebas.cache()

#crear el modelo
modelo=tf.keras.Sequential([
	tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28,1)),
	tf.keras.layers.MaxPooling2D(2, 2),


        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
	tf.keras.layers.MaxPooling2D(2, 2),

        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
	tf.keras.layers.MaxPooling2D(2, 2),

        tf.keras.layers.Dropout(0.5),
	tf.keras.layers.Flatten(),
	tf.keras.layers.Dense(512, activation='relu'),
	tf.keras.layers.Dense(10, activation=tf.nn.softmax) #Para redes de clasificacion
        ])


#Compilar el modelo
modelo.compile(
	optimizer='adam',
	loss=tf.keras.losses.SparseCategoricalCrossentropy(),
	metrics=['accuracy'])

#Los numeros de datos en entrenamiento y pruebas (60k y 10k)
num_ej_entrenamiento=metadatos.splits["train"].num_examples
num_ej_pruebas=metadatos.splits["test"].num_examples

#El trabajo por lotes permite que entrenamientos con gran cantidad de datos se haga de mas eficiente
TAMANO_LOTE=32

#Shufle y repeat hacen que los datos esten mezclados de manera aleatoria para que la red
#no se vaya a aprender el orden de las cosas
datos_entrenamiento=datos_entrenamiento.repeat().shuffle(num_ej_entrenamiento).batch(TAMANO_LOTE)
datos_pruebas=datos_pruebas.batch(TAMANO_LOTE)

import math
# entrenar

historial = modelo.fit( datos_entrenamiento, epochs=5, steps_per_epoch=math.ceil(num_ej_entrenamiento/TAMANO_LOTE) )

# Guardar en formato SavedModel
modelo.export('numeros_tfjs')

# Instalar librería de conversión
!pip install tensorflowjs

# Crear carpeta de salida
!mkdir carpeta_salida

# Convertir a TensorFlow.js GraphModel
!tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model numeros_tfjs carpeta_salida

# Confirmar que se generaron los archivos
!ls carpeta_salida